{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import h5py\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic pytorch setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Connect torch to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "f = h5py.File('data/sdr_wifi_test.hdf5', 'r')\n",
    "X_test = f['X'][()]\n",
    "y_test = f['y'][()]\n",
    "f.close()\n",
    "\n",
    "# Load train set\n",
    "f = h5py.File('data.sdr_wifi_train.hdf5', 'r')\n",
    "X_train = f['X'][()]\n",
    "y_train = f['y'][()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of input variables\n",
    "\n",
    "- `input_size` is the amount of frequencies that have been measured\n",
    "- `hidden_size` is the amount of neurons in the hidden layer\n",
    "- `num_layers` is the amount of hidden layers that perform changes to get the correct prediction (chosen based on https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw)\n",
    "- `output_size` is the amount of frequencies that are predicted, these are equal to the input since we want to see from the full prediction which frequency most likely has the lowest interference\n",
    "- `seq_length` is the amount of history that gets taken into account to make the new prediction\n",
    "- `num_epochs` is the amount of training rounds\n",
    "- `learning_rate` is the rate at which the weights of the hidden layers are updated to improve prediction results (cannot be too high because it might overshoot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1439971, 4, 29])\n",
      "torch.Size([1439971, 4])\n"
     ]
    }
   ],
   "source": [
    "data = torch.FloatTensor(X_train).to(device)\n",
    "labels = torch.FloatTensor(y_train).to(device)\n",
    "\n",
    "print(data.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "# Neural network input\n",
    "input_size = data.shape[1]\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "output_size = labels.shape[1]\n",
    "\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 5\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Fully connected on the last output\n",
    "        out = self.sigmoid(out)  # Apply sigmoid activation\n",
    "        return out\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LSTMModel(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "criterion = nn.MSELoss()  # Binary Cross Entropy Loss for outputs between 0 and 1\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], (1439936 / 1439971) Loss: 0.0041"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        # Get batch\n",
    "        x_batch = data[i:i+batch_size]\n",
    "        y_batch = labels[i:i+batch_size]\n",
    "\n",
    "        x_batch = x_batch.permute(0, 2, 1)  # Reshape to (batch_size, seq_len, input_size)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        sys.stdout.write(f\"\\rEpoch [{epoch+1}/{num_epochs}], ({i} / {len(data)}) Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0084\n",
      "Total Test Loss: 13.430269373166084\n",
      "Test Accuracy: 0.9931\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i in range(0, len(X_test), batch_size):\n",
    "        X_batch = torch.FloatTensor(X_test[i:i+batch_size]).to(device)\n",
    "        y_batch = torch.FloatTensor(y_test[i:i+batch_size]).to(device)\n",
    "\n",
    "        X_batch = X_batch.permute(0, 2, 1)  # Reshape to (batch_size, seq_len, input_size)\n",
    "\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Convert probabilities to binary predictions\n",
    "        predicted = (outputs > 0.5).float()\n",
    "\n",
    "        # Calculate the number of correct predictions\n",
    "        correct_predictions += (predicted == y_batch).sum().item()\n",
    "        total_samples += y_batch.numel()\n",
    "\n",
    "        sys.stdout.write(f\"\\rTest Loss: {loss.item():.4f}\")\n",
    "\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    print(f\"\\nTotal Test Loss: {total_loss}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([[1., 0., 0., 1.]], device='cuda:0')\n",
      "Actual: [0 0 1 1]\n",
      "Output: tensor([[0.9479, 0.1858, 0.3457, 0.9773]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction based on the last sample\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_sample = torch.FloatTensor(X_test[-1]).to(device)\n",
    "    X_sample = X_sample.view(1, X_sample.shape[1], X_sample.shape[0])\n",
    "    output = model(X_sample)\n",
    "\n",
    "    predicted = (output > 0.5).float()\n",
    "\n",
    "    print(f\"Predicted: {predicted}\")\n",
    "    print(f\"Actual: {y_test[-1]}\")\n",
    "    print(f\"Output: {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
